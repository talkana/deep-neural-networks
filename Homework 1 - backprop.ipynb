{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Checkpointing\n",
    "\n",
    "Your task is to implement checkpointing for a MLP using NumPy.\n",
    "\n",
    "You are free to use the implementation of a MLP and the backpropagation algorithm that you have developed during lab sessions.\n",
    "\n",
    "The key takeaway from this task is that with checkpointing we can trade off the computational resources needed to compute the forward pass of the network for the memory requirement needed to perform a backward pass in the network, which is often a major bottleneck when training large networks. In plain english, we can slightly increase the time required for training our network to save some of our GPU's precious memory.\n",
    "\n",
    "## What is checkpointing?\n",
    "\n",
    "The aim of checkpointing is to save every $n$-th layer's (e.g. every 2-nd layer's) forward result (instead of saving every layer's forward result as in plain backpropagation) and use these checkpoints for recomputing the forward pass of the network upon doing a backward pass. Checkpoint layers are kept in memory after the forward pass, while the remaining activations are recomputed at most once. After being recomputed, the non-checkpoint layers are kept in memory until they are no longer required."
   ],
   "metadata": {
    "collapsed": false,
    "id": "wgLdGm9y6LSE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What should be done\n",
    "\n",
    "1. Take the implementation a MLP trained with backpropagation. Analyze the algorithm with respect to the memory that is used by the algorithm with respect to the number of hidden layers.\n",
    "\n",
    "2. Implement a class NetworkWithCheckpointing that inherits from the Network class defined during lab sessions by:\n",
    "    a) implementing a method `forward_between_checkpoints` that will recompute the forward pass of the network using one of the checkpointed layers\n",
    "    b) override the method `backprop` to use only checkpointed layers and otherwise compute the activations using `forward_between_checkpoints` method and keep it in memory until no longer needed.\n",
    "\n",
    "3. Train your network with checkpoinintg on MNIST. Compare running times and memory usage with respect to the network without checkpointing.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "dkt4bf7b6LSM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implement Checkpointing for a MLP"
   ],
   "metadata": {
    "collapsed": false,
    "id": "gLCe_gBI6LSN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Take the implementation a MLP trained with backpropagation. Analyze the algorithm with respect to the memory that is used by the algorithm with respect to the number of hidden layers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Let's read the mnist dataset\n",
    "\n",
    "def load_mnist(path='mnist.npz'):\n",
    "    with np.load(path) as f:\n",
    "        x_train, _y_train = f['x_train'], f['y_train']\n",
    "        x_test, _y_test = f['x_test'], f['y_test']\n",
    "\n",
    "    x_train = x_train.reshape(-1, 28 * 28) / 255.\n",
    "    x_test = x_test.reshape(-1, 28 * 28) / 255.\n",
    "\n",
    "    y_train = np.zeros((_y_train.shape[0], 10))\n",
    "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
    "\n",
    "    y_test = np.zeros((_y_test.shape[0], 10))\n",
    "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist()"
   ],
   "metadata": {
    "id": "wuf6MFeE6LSR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    # Derivative of the sigmoid\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        # initialize biases and weights with random normal distr.\n",
    "        # weights are indexed by target node first\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        np.random.seed(17)\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        np.random.seed(18)\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        # Run the network on a single case\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "\n",
    "    def update_mini_batch(self, x_mini_batch, y_mini_batch, eta):\n",
    "        # Update networks weights and biases by applying a single step\n",
    "        # of gradient descent using backpropagation to compute the gradient.\n",
    "        # The gradient is computed for a mini_batch.\n",
    "        # eta is the learning rate\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in zip(x_mini_batch, y_mini_batch):\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x.reshape(784, 1), y.reshape(10, 1))\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w - (eta / len(x_mini_batch)) * nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / len(x_mini_batch)) * nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        # For a single input (x,y) return a tuple of lists.\n",
    "        # First contains gradients over biases, second over weights.\n",
    "\n",
    "        # First initialize the list of gradient arrays\n",
    "        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n",
    "        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n",
    "\n",
    "        # Then go forward remembering all values before and after activations\n",
    "        # in two other array lists\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = w @ activation + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # Now go backward from the final cost applying backpropagation\n",
    "        for l in range(1, self.num_layers):\n",
    "            if l == 1:\n",
    "                # chain rule: dC/db = dC/da * da/dz * dz/db = delta * dz/db\n",
    "                # dC/dw = dC/da * da/dz * dz/dw = delta * dz/dw\n",
    "                delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "            else:\n",
    "                # dC/dz_2 = dC/dz_1 * dz_1/da_2 * da_2/dz_2\n",
    "                delta = self.weights[-l + 1].transpose() @ delta * sigmoid_prime(zs[-l])\n",
    "            delta_nabla_b[-l] = delta\n",
    "            delta_nabla_w[-l] = delta @ activations[-l - 1].transpose()\n",
    "        return delta_nabla_b, delta_nabla_w\n",
    "\n",
    "    def evaluate(self, x_test_data, y_test_data):\n",
    "        # Count the number of correct answers for test_data\n",
    "        test_results = [(np.argmax(self.feedforward(x_test_data[i].reshape(784, 1))), np.argmax(y_test_data[i]))\n",
    "                        for i in range(len(x_test_data))]\n",
    "        # return accuracy\n",
    "        return np.mean([int(x == y) for (x, y) in test_results])\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return output_activations - y\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        x_train, y_train = training_data\n",
    "        if test_data:\n",
    "            x_test, y_test = test_data\n",
    "        for j in range(epochs):\n",
    "            for i in range(x_train.shape[0] // mini_batch_size):\n",
    "                x_mini_batch = x_train[i * mini_batch_size:(i * mini_batch_size + mini_batch_size)]\n",
    "                y_mini_batch = y_train[i * mini_batch_size:(i * mini_batch_size + mini_batch_size)]\n",
    "                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test, y_test)))\n",
    "            else:\n",
    "                print(\"Epoch: {0}\".format(j))\n"
   ],
   "metadata": {
    "id": "P4VVlL1R6LSS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a look at the backprop function. It is easy to notice that after the forward pass we keep in memory vectors of values before and after activations for each non-input layer. We also store a vector delta_nabla_b of size equal to the number of biases of the network and a vector delta_nabla_w of size equal to the number of weights of the network. All in all, for a network with k non-input layers, each containing at most n neurons, we use extra space equal to roughly 2n*k + biases.size() + weights.size().\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Implement a class NetworkWithCheckpointing that inherits from the Network class defined during lab sessions by:\n",
    "    a) implementing a method `forward_between_checkpoints` that will recompute the forward pass of the network using one of the checkpointed layers\n",
    "    b) override the method `backprop` to use only checkpointed layers and otherwise compute the activations using `forward_between_checkpoints` method and keep it in memory until no longer needed.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class NetworkWithCheckpointing(Network):\n",
    "\n",
    "    def __init__(self, sizes, checkpoint_every_nth_layer: int = 0, *args, **kwargs):\n",
    "        super().__init__(sizes, *args, **kwargs)\n",
    "        self.n = max(checkpoint_every_nth_layer, 1) # no checkpointing if input is 0 or smaller\n",
    "\n",
    "    def forward_between_checkpoints(self, a, checkpoint_idx_start, layer_idx_end, checkpoint=False):\n",
    "        \"\"\"\n",
    "        Given activations of a layer, the index of a starting layer and the index of the ending\n",
    "        layer, compute forward pass and save values before activation in a dict (layer index -> values).\n",
    "        If checkpoint is True, only every n-th value is saved\n",
    "        \"\"\"\n",
    "        zs = {}\n",
    "        i = checkpoint_idx_start\n",
    "        for b, w in zip(self.biases[checkpoint_idx_start - 1:layer_idx_end],\n",
    "                        self.weights[checkpoint_idx_start - 1:layer_idx_end]):\n",
    "            z = w @ a + b\n",
    "            a = sigmoid(z)\n",
    "            if not checkpoint or i % self.n == 0:\n",
    "                zs[i] = z\n",
    "            i += 1\n",
    "        return zs\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        def _compute_z(i, i_to_z, i_to_z_recomputed):\n",
    "            \"\"\" Given index of a target layer, a dictionary of checkpoint values and a dictionary\n",
    "            of values computed from last checkpoint, compute the values of the target layer before activation\"\"\"\n",
    "            if i in i_to_z.keys():\n",
    "                new_z = i_to_z[i]\n",
    "                del i_to_z[i]\n",
    "            elif i in i_to_z_recomputed:\n",
    "                new_z = i_to_z_recomputed[i]\n",
    "            else:\n",
    "                start_i = max(i_to_z.keys()) if len(i_to_z) else 0\n",
    "                start_value = x if start_i == 0 else sigmoid(i_to_z[start_i])\n",
    "                i_to_z_recomputed = self.forward_between_checkpoints(start_value, start_i + 1, i)\n",
    "                new_z = i_to_z_recomputed[i]\n",
    "            return new_z, i_to_z, i_to_z_recomputed\n",
    "\n",
    "        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n",
    "        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n",
    "\n",
    "        zs = self.forward_between_checkpoints(x, 1, self.num_layers - 1, True)\n",
    "        zs_recomputed = {}\n",
    "        z, zs, zs_recomputed = _compute_z(self.num_layers - 1, zs, zs_recomputed)\n",
    "        for l in range(1, self.num_layers):\n",
    "            if l == 1:\n",
    "                delta = self.cost_derivative(sigmoid(z), y) * sigmoid_prime(z)\n",
    "            else:\n",
    "                delta = self.weights[-l + 1].transpose() @ delta * sigmoid_prime(z)\n",
    "            if l != self.num_layers - 1:\n",
    "                next_z, zs, zs_recomputed = _compute_z(self.num_layers - l - 1, zs, zs_recomputed)\n",
    "                next_a = sigmoid(next_z)\n",
    "            else:\n",
    "                next_a = x\n",
    "            delta_nabla_b[-l] = delta\n",
    "            delta_nabla_w[-l] = delta @ next_a.transpose()\n",
    "            z = next_z\n",
    "        return delta_nabla_b, delta_nabla_w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Train your network with checkpoinintg on MNIST. Compare running times and memory usage with respect to the network without checkpointing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's first analyse memory usage. Similarly as in the network without checkpointing, we store vectors delta_nabla_b and delta_nabla_w with sizes equal to the numbers of biases and weights respectively. For a checkpointing parameter c and a number of layers k, we keep in memory at most k/c checkpointed vectors of values before activation. Moreover, at each time we keep in memory the number of recomputed vectors before activation bounded by c. All in all, for a network with k non-input layers, each containing at most n neurons and a checkpointing parameter c, we use extra space roughly equal to c*n + n*k/c + biases.size() + weights.size()."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now look at the running times. Since running times should be similar for different epochs, we will only run one epoch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.34\n",
      "Running time: 52.688164472579956\n"
     ]
    }
   ],
   "source": [
    "network = Network([784, 30, 30, 30, 30, 30, 30, 30, 30, 10])\n",
    "st = time.time()\n",
    "network.SGD((x_train, y_train), epochs=1, mini_batch_size=100, eta=3., test_data=(x_test, y_test))\n",
    "et = time.time()\n",
    "print(f\"Running time: {et-st}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Network with checkpoints every 2 layers---\n",
      "Epoch: 0, Accuracy: 0.34\n",
      "Running time: 65.82348823547363\n",
      "---Network with checkpoints every 3 layers---\n",
      "Epoch: 0, Accuracy: 0.34\n",
      "Running time: 65.10165405273438\n",
      "---Network with checkpoints every 4 layers---\n",
      "Epoch: 0, Accuracy: 0.34\n",
      "Running time: 67.4464328289032\n",
      "---Network with checkpoints every 5 layers---\n",
      "Epoch: 0, Accuracy: 0.34\n",
      "Running time: 64.5918242931366\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "for x in range(2, 6):\n",
    "    print(f\"---Network with checkpoints every {x} layers---\")\n",
    "    network = NetworkWithCheckpointing([784, 30, 30, 30, 30, 30, 30, 30, 30, 10], x)\n",
    "    st = time.time()\n",
    "    network.SGD((x_train, y_train), epochs=1, mini_batch_size=100, eta=3., test_data=(x_test, y_test))\n",
    "    et = time.time()\n",
    "    times.append(et-st)\n",
    "    print(f\"Running time: {et-st}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above example we see very similar running times for all checkpointing parameters in [2, 5]. The running times of the networks with checkpointing are larger compared to the regular network, but in general we expect them to be at most twice as large. That is because when using checkpointing we compute the values of each layer at most twice (once in the forward pass and once using the closest checkpoint)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}